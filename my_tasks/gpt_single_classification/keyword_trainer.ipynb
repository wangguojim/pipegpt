{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BartForSequenceClassification\n",
    "from config import get_config\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from keyword_process import MyDataset, DataLoader, paired_collate_fn\n",
    "from log import Logger\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.logger = Logger(log_path=config.save_path).logger\n",
    "        self.model = BartForSequenceClassification.from_pretrained(\n",
    "            config.base_params, num_labels=9,  problem_type='single_label_classification').to(config.device)\n",
    "        self.scheduler = ''\n",
    "        self.optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, self.model.parameters()), lr=config.lr)\n",
    "        self.criterion = nn.CrossEntropyLoss().to(config.device)\n",
    "        self.writer = SummaryWriter(config.tensorboard_path)\n",
    "\n",
    "    def _update(self, inputs, mode):\n",
    "        inputs = [i.to(self.config.device)if not isinstance(i, list) else i for i in inputs ]\n",
    "        enc_input_padded, enc_mask, labels = inputs\n",
    "        labels = torch.Tensor(labels).long().to(self.config.device)\n",
    "        out = self.model(enc_input_padded, enc_mask, labels=labels)\n",
    "        outputs, loss = out.logits, out.loss\n",
    "        num_corr, num = self._cal_preformance(outputs, labels)\n",
    "        if mode == 'train':\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        return loss.item(), num_corr, num\n",
    "\n",
    "    def _cal_preformance(self, logits, labels):\n",
    "        # mask = mask.unsqueeze(dim=-1).repeat(1, 1, logits.shape[-1])\n",
    "        # logits = logits.masked_select(mask).view(-1, logits.shape[-1])\n",
    "        # logits = logits.reshape(-1)\n",
    "        # labels = torch.Tensor(labels).to(self.config.device)\n",
    "        # loss = self.model(logits, labels=labels).loss\n",
    "        # loss = self.criterion(logits, labels)\n",
    "        num_corr = 0\n",
    "#         for i in range(logits.shape[0]):\n",
    "#             if ((logits[0] > 0.5) == labels[i]).all():\n",
    "#                 num_corr += 1\n",
    "        _, indices = logits.max(dim=1)\n",
    "        num_corr = indices.eq(labels).sum().item()\n",
    "        return num_corr, logits.shape[0]\n",
    "\n",
    "    def _run_epoch(self, dataset, mode):\n",
    "        if mode not in ['train', 'eval']:\n",
    "            raise Exception(\"you must select 'train' or 'eval' as a value of mode!\")\n",
    "        total_corr = 0  # 预测正确总个数\n",
    "        total_labs = 0  # 总标签数\n",
    "        total_loss = 0  # 总损失\n",
    "        total_num = 0  # 样本总个数\n",
    "        if mode == 'train':\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "        for idx, inputs in tqdm(enumerate(dataset), total=len(dataset)):\n",
    "            # if idx == 29: continue\n",
    "            # print(idx)\n",
    "\n",
    "            loss, num_corr, num = self._update(inputs, mode)\n",
    "\n",
    "            total_corr += num_corr\n",
    "            total_labs += num\n",
    "            total_loss += loss\n",
    "            total_num += 1\n",
    "\n",
    "            # if idx == 33: exit()\n",
    "        avg_loss = round(total_loss/(total_num), 5)  # 平均损失\n",
    "        accura = round(total_corr/(total_labs), 4)  # 准确率\n",
    "        # f1 = round(2*recall*accura/(recall+accura+1e-5), 2)  # f1\n",
    "        return avg_loss, accura\n",
    "\n",
    "    def train(self, num_eopch, train_dataset, val_dataset, save_path, checkpoint_path=False):\n",
    "        start_epoch = 0\n",
    "        if checkpoint_path:\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=self.config['device'])\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            assert start_epoch < num_eopch\n",
    "            self.model.load_state_dict(checkpoint['params'])\n",
    "\n",
    "        t_avg_loss, t_accura = self._run_epoch(train_dataset, 'eval')\n",
    "        e_avg_loss, e_accura = self._run_epoch(val_dataset, 'eval')\n",
    "        self.writer.add_scalars('show', {'train-loss': t_avg_loss,\n",
    "                                               'eval-loss': e_avg_loss,\n",
    "                                               'train-acc': t_accura,\n",
    "                                               'eval-acc': e_accura}\n",
    "                                       , 1)\n",
    "\n",
    "        self.logger.info(f'-Train  loss:{t_avg_loss}   accuracy:{t_accura * 100}%')\n",
    "        self.logger.info(f'-Eval   loss:{e_avg_loss}   accuracy:{e_accura * 100}%')\n",
    "        best_params, save_epoch = 0, 0\n",
    "        for epoch in range(start_epoch, num_eopch):\n",
    "            self.logger.info(f'Epoch {epoch+1}/{num_eopch}:')\n",
    "            t_avg_loss, t_accura = self._run_epoch(train_dataset, 'train')\n",
    "            e_avg_loss, e_accura = self._run_epoch(val_dataset, 'eval')\n",
    "            self.writer.add_scalars('show', {'train-loss': t_avg_loss,\n",
    "                                               'eval-loss': e_avg_loss,\n",
    "                                               'train-acc': t_accura,\n",
    "                                               'eval-acc': e_accura}\n",
    "                                       , epoch+2)\n",
    "            self.logger.info(f'-Train  loss:{t_avg_loss}   accuracy:{t_accura*100}%')\n",
    "            self.logger.info(f'-Eval   loss:{e_avg_loss}   accuracy:{e_accura*100}%')\n",
    "            if abs(t_accura-e_accura) <= 0.1 and t_accura >= 0.93 and e_accura >= 0.93:\n",
    "                if e_accura>best_params:\n",
    "                    best_params = e_accura\n",
    "                    model_state_dict = self.model.state_dict()\n",
    "                    checkpoint = {\n",
    "                        'params': model_state_dict,\n",
    "                        'configs': self.config,\n",
    "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                        'epoch': epoch}\n",
    "                    if not os.path.exists(save_path):\n",
    "                        os.mkdir(save_path)\n",
    "                    if os.path.exists(save_path + '/' + str(save_epoch) + '.pth'):\n",
    "                        os.remove(save_path + '/' + str(save_epoch) + '.pth')\n",
    "                        self.logger.info(f'the model saved in: {save_path}/{save_epoch}.pth  will be revomed')\n",
    "                    file_name = f'{save_path}/{epoch}.pth'\n",
    "                    self.logger.info(f'the model will be saved in: {file_name}')\n",
    "                    torch.save(checkpoint, file_name)\n",
    "                    save_epoch = epoch\n",
    "\n",
    "    def test(self, test_dataset, checkpoint_path=False):\n",
    "        if checkpoint_path:\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=self.config.device)\n",
    "            self.model.load_state_dict(checkpoint['params'])\n",
    "        avg_loss, accura = self._run_epoch(test_dataset, 'eval')\n",
    "        self.logger.info(f'-test   loss:{avg_loss}   accuracy:{accura * 100}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(base_params='model_path', batch_size=32, data_path='data/营业厅数字人项目事项v1.0-人工匹配规则枚举.xlsx', device='cuda:0', lr=1e-05, max_len=512, num_epoch=300, save_path='./ckpts/finetune_keyword', seed=1, split_rate=0.8, tensorboard_path='./ckpts/finetune_keyword_tensorboard')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at model_path and are newly initialized: ['classification_head.out_proj.bias', 'classification_head.out_proj.weight', 'classification_head.dense.bias', 'classification_head.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = get_config()\n",
    "# 定义Summary_Writer\n",
    "print(config)\n",
    "train_dataset = MyDataset(config.data_path, mode='train')\n",
    "eval_dataset =MyDataset(config.data_path, mode='test')\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=config.batch_size, collate_fn=paired_collate_fn)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=config.batch_size, collate_fn=paired_collate_fn)\n",
    "trainer = Trainer(config)\n",
    "# trainer.train(config.num_epoch, train_loader, eval_loader, config.save_path,)\n",
    "#                   checkpoint_path='./data/ckpts/fine_tuned/30.pth')\n",
    "# trainer.test(eval_loader, checkpoint_path='./ckpts/finetune2/7.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['换套餐',\n",
       " '多少分钟',\n",
       " '更 套餐',\n",
       " '通话时间',\n",
       " '啥套餐',\n",
       " '优惠',\n",
       " '套餐办理',\n",
       " '哪个套餐',\n",
       " '月租',\n",
       " '超出费用',\n",
       " '超出 资费',\n",
       " '通话多少',\n",
       " '分钟数',\n",
       " '超出收费',\n",
       " '超出 资费',\n",
       " '超出 收费',\n",
       " '什么流量包',\n",
       " '免流量',\n",
       " '限速吗',\n",
       " '行行行',\n",
       " '麻烦你了',\n",
       " '我要',\n",
       " '记一个',\n",
       " '哦那你说',\n",
       " '哎没问题',\n",
       " '好的我晓得了',\n",
       " '怎样操作',\n",
       " '没什么问题',\n",
       " '啊也行',\n",
       " '行的吧',\n",
       " '用',\n",
       " '好的好的好的',\n",
       " '哦没问题',\n",
       " '哦可以啊',\n",
       " '行没有问题',\n",
       " '你说吧',\n",
       " '哦好的',\n",
       " '嗯好',\n",
       " '好的呀',\n",
       " '好哦',\n",
       " '嗯行可以的',\n",
       " '嗯是的',\n",
       " '讲吧讲吧',\n",
       " '哎好的',\n",
       " '额好啊',\n",
       " '可以',\n",
       " '额可以',\n",
       " '办',\n",
       " '讲讲看',\n",
       " '嗯没问题',\n",
       " '对是',\n",
       " '嗯',\n",
       " '可以没有问题',\n",
       " '可以啊',\n",
       " '行行好的',\n",
       " '哦哦行啊',\n",
       " '可以啊没问题',\n",
       " '哎对',\n",
       " '哦哦哦',\n",
       " '说吧',\n",
       " '要用',\n",
       " '哦好',\n",
       " '使用',\n",
       " '嗯可以',\n",
       " '哦好好',\n",
       " '行呗',\n",
       " '木有兴趣',\n",
       " '没这想法',\n",
       " '我没需求',\n",
       " '不很想',\n",
       " '还没想法',\n",
       " '不办',\n",
       " '用不上',\n",
       " '不想',\n",
       " '不同意',\n",
       " '好的不用了',\n",
       " '不必要',\n",
       " '没这方面想法',\n",
       " '还是算了吧',\n",
       " '不做这个',\n",
       " '不会考虑',\n",
       " '这个不合适我',\n",
       " '那我不要',\n",
       " '没想过',\n",
       " '不去',\n",
       " '没这个需求',\n",
       " '不弄了',\n",
       " '没计划',\n",
       " '没想法',\n",
       " '不可以',\n",
       " '啊我不想',\n",
       " '没有什么需求',\n",
       " '没有这个想法',\n",
       " '真的没兴趣',\n",
       " '没有需求',\n",
       " '太麻烦不要',\n",
       " '没这方面需要',\n",
       " '下次在合作',\n",
       " '都不用',\n",
       " '我不想了解',\n",
       " '不给',\n",
       " '现在不考虑',\n",
       " '没做了',\n",
       " '不了',\n",
       " '不搞了',\n",
       " '不用来',\n",
       " '没有想法',\n",
       " '现在不想做',\n",
       " '我没需要',\n",
       " '没这个需要',\n",
       " '没什么需求',\n",
       " '再一遍',\n",
       " '哇',\n",
       " '再说一遍可以吗',\n",
       " '没听见',\n",
       " '听得不清晰',\n",
       " '慢慢说',\n",
       " '刚刚没听清',\n",
       " '没听清',\n",
       " '再重复说一遍',\n",
       " '我有点听不见',\n",
       " '再告诉我一遍',\n",
       " '再复述一遍',\n",
       " '你再说一次',\n",
       " '听不太清楚',\n",
       " '再说一下',\n",
       " '再重复说一下',\n",
       " '你在说什么',\n",
       " '没有听到',\n",
       " '吗',\n",
       " '可以再说一次吗',\n",
       " '不好意思再说一遍',\n",
       " '麻烦再讲一遍',\n",
       " '你刚才说了什么',\n",
       " '慢点讲',\n",
       " '你刚说了啥',\n",
       " '麻烦重说一遍',\n",
       " '重讲一次',\n",
       " '说什么刚才',\n",
       " '可以再说一下吗',\n",
       " '不好意思我没注意听',\n",
       " '你刚刚说啥',\n",
       " '好的呀',\n",
       " '对对是',\n",
       " '嗯好',\n",
       " '嗯行的',\n",
       " '嗯是',\n",
       " '好的吧',\n",
       " '好的好的没有问题',\n",
       " '哎对',\n",
       " '麻烦了',\n",
       " '介绍吧',\n",
       " '没什么问题了',\n",
       " '麻烦你了',\n",
       " '可以的啊',\n",
       " '嗯好可以的',\n",
       " '恩是的',\n",
       " '怎样办这个',\n",
       " '好的没的问题',\n",
       " '嗯你讲',\n",
       " '谢谢麻烦你了',\n",
       " '嗯嗯嗯',\n",
       " '行的',\n",
       " '哦好',\n",
       " '好的麻烦了',\n",
       " '对的',\n",
       " '可以登记',\n",
       " '咋办',\n",
       " '哦谢谢',\n",
       " '嗯可以可以',\n",
       " '额可以可以',\n",
       " '好好好',\n",
       " '嗯可以的',\n",
       " '唉好',\n",
       " '没的问题',\n",
       " '讲讲看',\n",
       " '需要',\n",
       " '嗯行我知道了',\n",
       " '咋搞',\n",
       " '好的好的',\n",
       " '有需要',\n",
       " '嗯嗯好的没问题',\n",
       " '怎么操作',\n",
       " '嗯啊',\n",
       " '行啊谢谢你',\n",
       " '可以可以没问题',\n",
       " '嗯没啥问题了',\n",
       " '嗯好好好',\n",
       " '你说一下',\n",
       " '好啊',\n",
       " '那行',\n",
       " '没有兴趣',\n",
       " '真的不想要',\n",
       " '不用了',\n",
       " '嗯不想',\n",
       " '啊我不想',\n",
       " '暂时没想法',\n",
       " '下次再合作',\n",
       " '没需求',\n",
       " '这方面没需求',\n",
       " '我不想',\n",
       " '还不考虑',\n",
       " '那我不要',\n",
       " '没有这个打算',\n",
       " '不打算考虑',\n",
       " '不大感兴趣',\n",
       " '不打算做',\n",
       " '不太想',\n",
       " '不要去想',\n",
       " '这个没兴趣',\n",
       " '好的用不到',\n",
       " '用不上',\n",
       " '暂时没需求',\n",
       " '没有这方面打算',\n",
       " '完全没兴趣',\n",
       " '没有',\n",
       " '没考虑',\n",
       " '没这想法',\n",
       " '不准备',\n",
       " '没有这个需要',\n",
       " '没这个需要',\n",
       " '好的不用了',\n",
       " '不给',\n",
       " '没有时间',\n",
       " '还是不了',\n",
       " '不想用',\n",
       " '不开通',\n",
       " '不行',\n",
       " '不办',\n",
       " '一点兴趣都没',\n",
       " '目前没需要',\n",
       " '不乐意',\n",
       " '不申请',\n",
       " '没想过',\n",
       " '没这个意向',\n",
       " '真的不需要',\n",
       " '主屏幕',\n",
       " '上面']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = [i[0] for i in train_dataset]\n",
    "test = [i[0] for i in eval_dataset]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:00<00:00, 122604.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "count = 0\n",
    "for i in tqdm(test):\n",
    "    if i in train : \n",
    "        count+= 1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qingquan",
   "language": "python",
   "name": "qingquan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
