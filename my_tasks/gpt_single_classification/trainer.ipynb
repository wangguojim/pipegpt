{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BartForSequenceClassification\n",
    "from config import get_config\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from process import MyDataset, DataLoader, paired_collate_fn\n",
    "from log import Logger\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.logger = Logger(log_path=config.save_path).logger\n",
    "        self.model = BartForSequenceClassification.from_pretrained(\n",
    "            config.base_params, num_labels=9,  problem_type='single_label_classification').to(config.device)\n",
    "        self.scheduler = ''\n",
    "        self.optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, self.model.parameters()), lr=config.lr)\n",
    "        self.criterion = nn.CrossEntropyLoss().to(config.device)\n",
    "        self.writer = SummaryWriter(config.tensorboard_path)\n",
    "\n",
    "    def _update(self, inputs, mode):\n",
    "        inputs = [i.to(self.config.device)if not isinstance(i, list) else i for i in inputs ]\n",
    "        enc_input_padded, enc_mask, labels = inputs\n",
    "        labels = torch.Tensor(labels).long().to(self.config.device)\n",
    "        out = self.model(enc_input_padded, enc_mask, labels=labels)\n",
    "        outputs, loss = out.logits, out.loss\n",
    "        num_corr, num = self._cal_preformance(outputs, labels)\n",
    "        if mode == 'train':\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        return loss.item(), num_corr, num\n",
    "\n",
    "    def _cal_preformance(self, logits, labels):\n",
    "        # mask = mask.unsqueeze(dim=-1).repeat(1, 1, logits.shape[-1])\n",
    "        # logits = logits.masked_select(mask).view(-1, logits.shape[-1])\n",
    "        # logits = logits.reshape(-1)\n",
    "        # labels = torch.Tensor(labels).to(self.config.device)\n",
    "        # loss = self.model(logits, labels=labels).loss\n",
    "        # loss = self.criterion(logits, labels)\n",
    "        num_corr = 0\n",
    "#         for i in range(logits.shape[0]):\n",
    "#             if ((logits[0] > 0.5) == labels[i]).all():\n",
    "#                 num_corr += 1\n",
    "        _, indices = logits.max(dim=1)\n",
    "        num_corr = indices.eq(labels).sum().item()\n",
    "        return num_corr, logits.shape[0]\n",
    "\n",
    "    def _run_epoch(self, dataset, mode):\n",
    "        if mode not in ['train', 'eval']:\n",
    "            raise Exception(\"you must select 'train' or 'eval' as a value of mode!\")\n",
    "        total_corr = 0  # 预测正确总个数\n",
    "        total_labs = 0  # 总标签数\n",
    "        total_loss = 0  # 总损失\n",
    "        total_num = 0  # 样本总个数\n",
    "        if mode == 'train':\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "        for idx, inputs in tqdm(enumerate(dataset), total=len(dataset)):\n",
    "            # if idx == 29: continue\n",
    "            # print(idx)\n",
    "\n",
    "            loss, num_corr, num = self._update(inputs, mode)\n",
    "\n",
    "            total_corr += num_corr\n",
    "            total_labs += num\n",
    "            total_loss += loss\n",
    "            total_num += 1\n",
    "\n",
    "            # if idx == 33: exit()\n",
    "        avg_loss = round(total_loss/(total_num), 5)  # 平均损失\n",
    "        accura = round(total_corr/(total_labs), 4)  # 准确率\n",
    "        # f1 = round(2*recall*accura/(recall+accura+1e-5), 2)  # f1\n",
    "        return avg_loss, accura\n",
    "\n",
    "    def train(self, num_eopch, train_dataset, val_dataset, save_path, checkpoint_path=False):\n",
    "        start_epoch = 0\n",
    "        if checkpoint_path:\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=self.config['device'])\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            assert start_epoch < num_eopch\n",
    "            self.model.load_state_dict(checkpoint['params'])\n",
    "\n",
    "        t_avg_loss, t_accura = self._run_epoch(train_dataset, 'eval')\n",
    "        e_avg_loss, e_accura = self._run_epoch(val_dataset, 'eval')\n",
    "        self.writer.add_scalars('show', {'train-loss': t_avg_loss,\n",
    "                                               'eval-loss': e_avg_loss,\n",
    "                                               'train-acc': t_accura,\n",
    "                                               'eval-acc': e_accura}\n",
    "                                       , 1)\n",
    "\n",
    "        self.logger.info(f'-Train  loss:{t_avg_loss}   accuracy:{t_accura * 100}%')\n",
    "        self.logger.info(f'-Eval   loss:{e_avg_loss}   accuracy:{e_accura * 100}%')\n",
    "        best_params, save_epoch = 0, 0\n",
    "        for epoch in range(start_epoch, num_eopch):\n",
    "            self.logger.info(f'Epoch {epoch+1}/{num_eopch}:')\n",
    "            t_avg_loss, t_accura = self._run_epoch(train_dataset, 'train')\n",
    "            e_avg_loss, e_accura = self._run_epoch(val_dataset, 'eval')\n",
    "            self.writer.add_scalars('show', {'train-loss': t_avg_loss,\n",
    "                                               'eval-loss': e_avg_loss,\n",
    "                                               'train-acc': t_accura,\n",
    "                                               'eval-acc': e_accura}\n",
    "                                       , epoch+2)\n",
    "            self.logger.info(f'-Train  loss:{t_avg_loss}   accuracy:{t_accura*100}%')\n",
    "            self.logger.info(f'-Eval   loss:{e_avg_loss}   accuracy:{e_accura*100}%')\n",
    "            if abs(t_accura-e_accura) <= 0.1 and t_accura >= 0.93 and e_accura >= 0.93:\n",
    "                if e_accura>best_params:\n",
    "                    best_params = e_accura\n",
    "                    model_state_dict = self.model.state_dict()\n",
    "                    checkpoint = {\n",
    "                        'params': model_state_dict,\n",
    "                        'configs': self.config,\n",
    "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                        'epoch': epoch}\n",
    "                    if not os.path.exists(save_path):\n",
    "                        os.mkdir(save_path)\n",
    "                    if os.path.exists(save_path + '/' + str(save_epoch) + '.pth'):\n",
    "                        os.remove(save_path + '/' + str(save_epoch) + '.pth')\n",
    "                        self.logger.info(f'the model saved in: {save_path}/{save_epoch}.pth  will be revomed')\n",
    "                    file_name = f'{save_path}/{epoch}.pth'\n",
    "                    self.logger.info(f'the model will be saved in: {file_name}')\n",
    "                    torch.save(checkpoint, file_name)\n",
    "                    save_epoch = epoch\n",
    "\n",
    "    def test(self, test_dataset, checkpoint_path=False):\n",
    "        if checkpoint_path:\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=self.config.device)\n",
    "            self.model.load_state_dict(checkpoint['params'])\n",
    "        avg_loss, accura = self._run_epoch(test_dataset, 'eval')\n",
    "        self.logger.info(f'-test   loss:{avg_loss}   accuracy:{accura * 100}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(base_params='model_path', batch_size=32, data_path='data/营业厅数字人项目事项v1.0-人工匹配规则枚举.xlsx', device='cuda:0', lr=1e-05, max_len=512, num_epoch=300, save_path='./ckpts/finetune_4', seed=1, split_rate=0.8, tensorboard_path='./ckpts/finetune_4_tensorboard')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at model_path and are newly initialized: ['classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight', 'classification_head.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = get_config()\n",
    "# 定义Summary_Writer\n",
    "print(config)\n",
    "train_dataset = MyDataset(config.data_path, mode='train')\n",
    "eval_dataset =MyDataset(config.data_path, mode='test')\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=config.batch_size, collate_fn=paired_collate_fn)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=config.batch_size, collate_fn=paired_collate_fn)\n",
    "trainer = Trainer(config)\n",
    "# trainer.train(config.num_epoch, train_loader, eval_loader, config.save_path,)\n",
    "#                   checkpoint_path='./data/ckpts/fine_tuned/30.pth')\n",
    "# trainer.test(eval_loader, checkpoint_path='./ckpts/finetune2/7.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1638"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = [i[0] for i in train_dataset]\n",
    "test = [i[0] for i in eval_dataset]\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1638/1638 [00:00<00:00, 14855.83it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "count = 0\n",
    "for i in tqdm(test):\n",
    "    if i in train : \n",
    "        count+= 1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}